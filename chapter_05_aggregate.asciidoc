[[chapter_05_aggregate]]
== Aggregates And Consistency Boundaries

What's the point of a domain model anyway? What's the fundamental problem
we're trying to addresss?

Couldn't we just run everything in a spreadsheet? Many of our users would be
delighted by that. Business users _like_ spreadsheets because they're simple and
familiar but offer .

In fact, an enormous number of business processes do operate by manually sending
spreadsheets back and forward over e-mail. This "csv over smtp" architecture has
low initial complexity but tends not to scale very well because it's difficult
to apply logic.

// TODO: better examples.
Who is allowed to view this particular field? Who's allowed to update it? What
happens when we try to order -350 chairs, or 10,000,000 tables? Can an employee
have a negative salary?

These are the constraints of a system. Much of the domain logic we write exists
to enforce these constraints in order to maintain the _invariants_ of the
system. The invariants are the things that have to be true whenever we finish
an operation.

If we were writing a hotel booking system, we might have the constraint that no
two bookings can exist for the same hotel room on the same night. This supports
the invariant that no room is double booked.

Of course, sometimes we might need to temporarily *bend* the rules. Perhaps we
need to shuffle the rooms around due to a VIP booking. While we're moving people
around, we might be double booked, but our domain model should ensure that we
end up in a final consistent state, where the invariants are met.

Let's look at a concrete example:

[quote, Us, This book]
----

* I can't allocate to a batch if the available quantity is less than the
  quantity of the order line.
  Example: I have a batch of 1 BLUE-CUSHION, and an order line for 2
  BLUE-CUSHION. I should not be able to allocate the line to the batch.

----

This is a business rule that implements a constraint. The constraint is that we
can't allocate more stock than is available to a batch. The invariant is that
we never oversell stock by allocating two customers to the same physical cushion.
Every time we update the state of the system, our code needs to ensure that we
don't break the invariants.

// TODO: Maybe this ought to be a sidebar? It's an important point but sits awkwardly
If we're unable to meet our invariants, we might choose to raise an error rather
than complete the operation.

=== Invariants And Concurrency

In a single threaded single user application it's easy for us to maintain the
invariants of our system. We can just allocate stock, one line at a time, and
raise an error if there's no stock available.

This gets much harder when we introduce the idea of concurrency. Suddenly we
might be allocating stock for multiple order lines simultaneously. We might even
be allocating order lines, while we're processing changes to the batches
themselves.

We usually solve this problem by applying locks to our database tables. This
prevents two operations happening simultaneously on the same row or same
table.

As we start to think about scaling up our app, we realise that our model
of allocating lines against all available batches may not scale.  If we've
got tens of thousands of orders per hour, and hundreds of thousands of
order lines, we can't hold a lock over the whole `batches` table for
every single one.


=== Choosing The Right Aggregate

[quote, Eric Evans, DDD blue book]
____
// We need an abstraction for encapsulating references within the model.
An AGGREGATE is a cluster of associated objects that we treat as a unit for the
purpose of data changes.
// Each AGGREGATE has a root and a boundary. The boundary
// defines what is inside the AGGREGATE. The root is a single, specific ENTITY
// contained in the AGGREGATE. The root is the only member of the AGGREGATE that
// outside objects are allowed to hold references to, although objects within the
// boundary may hold references to each other. ENTITIES other than the root have
// local identity, but that identity needs to be distinguishable only within the
// AGGREGATE, because no outside object can ever see it out of the context of the
// root ENTITY.
____

Even if it weren't for the data integrity concerns, as a model gets more complex
and grows more different Entity and Value Objects, all of which start pointing
to each other, it can be hard to keep track of who can modify what.  Especially
when we have _collections_ in the model like we do (our batches are a collection),
it's a good idea to nominate some entities to be the single entrypoint for
modifying their related objects.  It makes the system conceptually simpler
and easy to reason about if you nominate some objects to be in charge of consistency
for the others.

Some sort of `Order` object might suggest itself, but that's more about order lines,
and we're more concerned about something that provides some sort of conceptual unity
for collections of batches.

When we allocate an order line, we're actually only interested in all the batches
that have the same SKU as the order line.  Some sort of concept of `GlobalSkuStock`
or perhaps just simply `Product` -- after all, that was the first concept we
came across in our exploration of the domain language back in <<chapter_01_domain_model>>.

Let's go with that for now, and see how it looks


[[product_aggregate]]
.Our chosen Aggregate, Product (src/allocation/model.py)
====
[source,python]
[role="non-head"]
----
class Product:

    def __init__(self, sku: str, batches: List[Batch]):
        self.sku = sku  #<1>
        self.batches = batches  #<2>

    def allocate(self, line: OrderLine) -> str:  #<3>
        try:
            batch = next(
                b for b in sorted(self.batches) if b.can_allocate(line)
            )
            batch.allocate(line)
            return batch.reference
        except StopIteration:
            raise OutOfStock(f'Out of stock for sku {line.sku}')
----
====

<1> Product's main identifier is the `sku`
<2> It hold a reference to a collection of `batches` for that sku
<3> And finally, we can move the `allocate()` domain service to
    being a method on `Product`.  

//TODO: talk about magic methods on aggregates maybe?  ie, a non-aggregate entity
//      might have a __hash__ so that we can put it into a set, but because you
//      are never supposed to have a collection of aggregates, they could return
//      an error for __has__. or sumfink.


.Aggregates, Bounded Contexts and Microservices
*******************************************************************************
One of the most important contributions from Evans and the DDD community
is the concept of
https://martinfowler.com/bliki/BoundedContext.html[_Bounded Contexts_].

In essence, this was a reaction against attempts to capture entire businesses
into a single model. The word "customer" means different things to people
in sales, customer services, logistics, support, and so on.  Attributes
needed in one context are irrelevant in another; more perniciously, concepts
with the same name can have entirely different meanings in different contexts.
Rather than trying to build a single model (or class, or database) to capture
all the use cases, better to have several different models, draw boundaries
around each context, and handle the translation between different contexts
explicitly.

This concept translates very well to the world of microservices, where each
microservice is free to have its own concept of "customer", and rules for
translating that to and from other microservices it integrates with.

Whether or not you've got a microservices architecture, a key consideration
in choosing your aggregates is also choosing the bounded context that they
will operate in.  By restricting the context, you can keep your number of
aggregates low and their size manageable.

Once again we find ourselves forced to say that we can't give this issue
the treatment it deserves here, and we can only encourage you to read up on it
elsewhere.

//TODO more links or suggestions on where to read about bounded context?

*******************************************************************************


=== 1 Aggregate = 1 Repository

Once you define certain entities to be Aggregates, we need to apply the
rule that they are the only entities that are publicly accessible to the
outside world.  In other words, the only repositories we are allowed should
be repositories that return aggregates.

In our case, we'll switch from `BatchRepository` to `ProductRepository`:


[[new_uow_and_repository]]
.Our new UoW and Repository (unit_of_work.py and repository.py)
====
[source,python]
[role="skip"]
----
class _UnitOfWork:
    def __init__(self, session):
        self.session = session
        self.products = repository.ProductRepository(session)


#...

class ProductRepository:
    #...

    def get(self, sku):
        return self.session.query(model.Product).filter_by(sku=sku).first()
----
====

And our service layer evolves to use `Product` as its main entrypoint:

[[service_layer_uses_products]]
.Service layer  (src/allocation/services.py)
====
[source,python]
----
def add_batch(
        ref: str, sku: str, qty: int, eta: Optional[date],
        uow: unit_of_work.AbstractUnitOfWork
):
    with uow:
        product = uow.products.get(sku=sku)
        if product is None:
            product = model.Product(sku, batches=[])
            uow.products.add(product)
        product.batches.append(model.Batch(ref, sku, qty, eta))
        uow.commit()


def allocate(
        orderid: str, sku: str, qty: int,
        uow: unit_of_work.AbstractUnitOfWork
) -> str:
    line = OrderLine(orderid, sku, qty)
    with uow:
        product = uow.products.get(sku=line.sku)
        if product is None:
            raise InvalidSku(f'Invalid sku {line.sku}')
        batchref = product.allocate(line)
        uow.commit()
    return batchref
----
====

TODO: discuss, should repository raise `InvalidSku`?


.Exercise for the Reader
******************************************************************************
You've just seen the main top layers of the code, so this shouldn't be too hard,
but we'd like you to implement the `Product` aggregate starting from `Batch`,
just like we did.

Of course you could cheat and copy/paste from the listings above, but even
if you do that, you'll still have to solve a few challenges on your own,
like adding the model to the ORM and making sure all the moving parts can
talk to each other, which we hope will be instructive.

https://github.com/python-leap/code/tree/chapter_05_aggregate_exercise

We've put in a "cheating" implementation in that delegates to the existing
`allocate()` function, so you should be able to evolve that towards the real
thing.

We've marked a couple of tests with `@pytest.skip()`, come back to then
when you're done and you've read the rest of this chapter, to have a go
at implementing version numbers.  Bonus points if you can get SQLAlchemy to
do them for you by magic!

******************************************************************************

=== Version Numbers

We've got our new aggregate and we're using it in all the right places, the remaining
question is:  how will we actually enforce our data integrity rules?  We don't want
to hold a lock over the entire batches table, but how will we implement holding a
lock over just the rows for a particular sku?  The answer is to have a single
attribute on the Product model which acts as a marker for the whole state change
being complete, and we use it as the single resource that concurrent workers
can fight over:  if two transactions both read the state of the world for `batches`
at the same time, and they both want to update the `allocations` tables, we force
both of them to also try and update the `version_number` in the `products` table,
in such a way that only one of them can win and the world stays consistent.

There are essentially 3 options for implementing version numbers:

1. `version_number` lives in domain, we add it to the `Product` constructor,
   and `Product.allocate()` is responsible for incrementing it.

2. The services layer could do it!  The version number isn't _strictly_ a domain
   concern, so instead our service layer could assume that the current version number 
   is attached to `Product` by the repository, and the service layer will increment it
   before it does the `commit()`

3. Or, since it's arguably an infrastructure concern, the UoW and repository
   could do it by magic.  The repository has access to version numbers for any
   products it retrieves, and when the UoW does a commit, it can increment the
   version number for any products it knows about, assuming them to have changed.


Option 3 isn't ideal, because there's no real way of doing it without having to
assume that _all_ products have changed, so we'll be incrementing version numbers
when we don't have tofootnote:[perhaps we could get some ORM/sqlalchemy magic to tell
us when an object is dirty, but how would that work in the generic case, eg for a
CsvRepository?].

Option 2 involves mixing the responsibility for mutating state between the service
layer and the domain layer, so it's a little messy as well.

So in the end, even though version numbers don't _have_ to be a domain concern,
you might decide the cleanest tradeoff is to put them in the domain.

[[product_aggregate_with_version_number]]
.Our chosen Aggregate, Product (src/allocation/model.py)
====
[source,python]
----
class Product:

    def __init__(self, sku: str, batches: List[Batch], version_number: int = 0):  #<1>
        self.sku = sku
        self.batches = batches
        self.version_number = version_number  #<1>

    def allocate(self, line: OrderLine) -> str:
        try:
            batch = next(
                b for b in sorted(self.batches) if b.can_allocate(line)
            )
            batch.allocate(line)
            self.version_number += 1  #<1>
            return batch.reference
        except StopIteration:
            raise OutOfStock(f'Out of stock for sku {line.sku}')
----
====

<1> There it is!

TODO: more discussion of version number -- actual numebr doesn't matter,
    we're just setting _something_ so the db complains, could use uids,
    also discuss similarity with eventsourcing version numbers.

=== Testing For Our Data Integrity Rules

Now to actually make sure we can get the behaviour we want: if we have two
concurrent attempts to do allocation against the same `Product`, one of them
should fail, because they can't both update the version number:

////
TODO:
In Example 5. An integration test for concurrency behaviour (tests/integration/test_uow.py)
it might be helpful to use order1 and order2 instead of o1 and o2.

This might have been morning-brain, but I had to read the code over a few times to figure out why product version was 4 instead of 1 or 2.
Perhaps instead something like:

product_version = 3
insert_batch(session, batch, sku, 100, eta=None, product_version=product_version)
...
assert version == 4
...

Or if you're ok leaving the constant behind:

...
assert version == product_version +1
...
https://github.com/python-leap/book/issues/36
////
[[data_integrity_test]]
.An integration test for concurrency behaviour (tests/integration/test_uow.py)
====
[source,python]
----
def test_concurrent_updates_to_version_are_not_allowed(postgres_session_factory):
    sku, batch = random_ref('s'), random_ref('b')
    session = postgres_session_factory()
    insert_batch(session, batch, sku, 100, eta=None, product_version=3)
    session.commit()

    exceptions = []
    o1, o2 = random_ref('o1'), random_ref('o2')
    target1 = lambda: try_to_allocate(o1, sku, exceptions)
    target2 = lambda: try_to_allocate(o2, sku, exceptions)
    t1 = threading.Thread(target=target1)  #<1>
    t2 = threading.Thread(target=target2)  #<1>
    t1.start()
    t2.start()
    t1.join()
    t2.join()

    [[version]] = session.execute(
        "SELECT version_number FROM products WHERE sku=:sku",
        dict(sku=sku),
    )
    assert version == 4  #<2>
    exception = [exceptions]
    assert 'could not serialize access due to concurrent update' in str(exception)  #<3>

    orders = list(session.execute(
        "SELECT orderid FROM allocations"
        " JOIN batches ON allocations.batch_id = batches.id"
        " JOIN order_lines ON allocations.orderline_id = order_lines.id"
        " WHERE order_lines.sku=:sku",
        dict(sku=sku),
    ))
    assert len(orders) == 1  #<4>
----
====

<1> We set up two threads that will reliably produce the concurrency behaviour we
    want:  `read1, read2, write1, write2`. (see below for the code being run in
    each thread).

<2> We assert that the version number has only been incremented once.

<3> We can also check on the specific exception if we like

<4> And we can make sure that only one allocation has gotten through.


[[time_sleep_thread]]
.time.sleep can reliably produce concurrency behaviour (tests/integration/test_uow.py)
====
[source,python]
----
def try_to_allocate(orderid, sku, exceptions):
    line = model.OrderLine(orderid, sku, 10)
    try:
        with unit_of_work.SqlAlchemyUnitOfWork() as uow:
            product = uow.products.get(sku=sku)
            product.allocate(line)
            time.sleep(0.2)
            uow.commit()
    except Exception as e:
        print(traceback.format_exc())
        exceptions.append(e)
----
====


==== Enforcing Concurrency Rules By Using Database Transaction Isolation Levels

To get the test to pass as it is, we can set the transaction isolation level
on our session:

[[transaction_serializable]]
.Set isolation level for session (src/allocation/unit_of_work.py)
====
[source,python]
----
DEFAULT_SESSION_FACTORY = sessionmaker(bind=create_engine(
    config.get_postgres_uri(),
    isolation_level="SERIALIZABLE",
))
----
====

Transaction isolation levels are tricky stuff, it's worth spending time
understanding https://www.postgresql.org/docs/9.6/transaction-iso.html[the
documentation].


==== SELECT FOR UPDATE Can Also Help

An alternative to using the `SERIALIZABLE` isolation level is to use
https://www.postgresql.org/docs/9.6/explicit-locking.html[SELECT FOR UPDATE],
which will produce different behaviour: two concurrent transactions will not
be allowed to do a read on the same rows at the same time.

[[with_for_update]]
.SqlAlchemy with_for_update (src/allocation/repository.py)
====
[source,python]
[role="non-head"]
----
    def get(self, sku):
        return self.session.query(model.Product) \
                           .filter_by(sku=sku) \
                           .with_for_update() \
                           .first()
----
====


This will have the effect of changing the concurrency pattern from 

[role="skip"]
----
read1, read2, write1, write2(fail)
----

to

[role="skip"]
----
read1, write1, read2, write2(succeed)
----

//TODO maybe better diagrams here?

In our simple case, it's not obvious which to prefer.  In a more complex
scenario, `SELECT FOR UPDATE` might lead to more deadlocks, while `SERIALIZABLE`
having more of an "optimistic locking" approach and might lead to more failures,
but the failures might be more recoverable.  So, as usual, the right solution
will depend on circumstances.


.Recap: Aggregates and consistency boundaries
*****************************************************************
Choose the right aggregate::
    bla

Something something transactions::
    bla bla.

*****************************************************************
